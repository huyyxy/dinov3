#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This software may be used and distributed in accordance with
# the terms of the DINOv3 License Agreement.

"""
è¡€ç»†èƒåˆ†ç±»æ¨ç†è„šæœ¬ - ä½¿ç”¨DINOv3ç‰¹å¾ + éšæœºæ£®æ—åˆ†ç±»å™¨
æ”¯æŒä¸¤ç§æ¨¡å¼:
1. å•å¼ å›¾åƒçš„å®Œæ•´æ£€æµ‹ï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£ï¼‰
2. å¯¹å·²æœ‰æ ‡æ³¨æ¡†è¿›è¡Œåˆ†ç±»
"""

import argparse
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter
import pickle
import xml.etree.ElementTree as ET

import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from torchvision import transforms as T
from tqdm import tqdm
import joblib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# å…¨å±€é…ç½®
COLORS = {
    'Platelets': (255, 0, 0),      # çº¢è‰² - è¡€å°æ¿
    'RBC': (0, 255, 0),            # ç»¿è‰² - çº¢ç»†èƒ
    'WBC': (0, 0, 255),            # è“è‰² - ç™½ç»†èƒ
}


# ============================================================================
# 1. æ•°æ®å¤„ç†ç›¸å…³å‡½æ•°
# ============================================================================

def parse_xml_annotation(xml_file: Path) -> List[Dict]:
    """è§£æPASCAL VOCæ ¼å¼çš„XMLæ ‡æ³¨æ–‡ä»¶
    
    Args:
        xml_file: XMLæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ ‡æ³¨å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«class_nameå’Œbbox
    """
    tree = ET.parse(xml_file)
    root = tree.getroot()
    
    objects = []
    for obj in root.findall('object'):
        class_name = obj.find('name').text
        bbox = obj.find('bndbox')
        xmin = int(bbox.find('xmin').text)
        ymin = int(bbox.find('ymin').text)
        xmax = int(bbox.find('xmax').text)
        ymax = int(bbox.find('ymax').text)
        
        objects.append({
            'class_name': class_name,
            'bbox': (xmin, ymin, xmax, ymax)
        })
    
    return objects


def clip_bbox_to_image(bbox: Tuple[int, int, int, int], 
                       image_size: Tuple[int, int]) -> Optional[Tuple[int, int, int, int]]:
    """å°†è¾¹ç•Œæ¡†è£å‰ªåˆ°å›¾åƒèŒƒå›´å†…
    
    Args:
        bbox: è¾¹ç•Œæ¡† (xmin, ymin, xmax, ymax)
        image_size: å›¾åƒå°ºå¯¸ (width, height)
        
    Returns:
        è£å‰ªåçš„è¾¹ç•Œæ¡†ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
    """
    xmin, ymin, xmax, ymax = bbox
    img_width, img_height = image_size
    
    xmin = max(0, min(xmin, img_width))
    ymin = max(0, min(ymin, img_height))
    xmax = max(0, min(xmax, img_width))
    ymax = max(0, min(ymax, img_height))
    
    # æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆ
    if xmax <= xmin or ymax <= ymin:
        return None
    
    return (xmin, ymin, xmax, ymax)


def generate_sliding_windows(image_size: Tuple[int, int], 
                            scales: Optional[List[int]] = None,
                            min_size: int = 20) -> List[Tuple[int, int, int, int]]:
    """ç”Ÿæˆå¤šå°ºåº¦æ»‘åŠ¨çª—å£çš„è¾¹ç•Œæ¡†
    
    Args:
        image_size: å›¾åƒå°ºå¯¸ (width, height)
        scales: çª—å£å°ºåº¦åˆ—è¡¨
        min_size: æœ€å°çª—å£å¤§å°
        
    Returns:
        è¾¹ç•Œæ¡†åˆ—è¡¨ [(xmin, ymin, xmax, ymax), ...]
    """
    width, height = image_size
    
    # é»˜è®¤å°ºåº¦
    if scales is None:
        scales = [32, 48, 64, 96, 128]
    
    windows = []
    
    for scale in scales:
        if scale < min_size:
            continue
        
        # è®¡ç®—æ­¥é•¿ï¼ˆçª—å£å¤§å°çš„ä¸€åŠï¼‰
        step = max(scale // 2, 16)
        
        # ç”Ÿæˆè¯¥å°ºåº¦çš„æ‰€æœ‰çª—å£
        for y in range(0, height - scale + 1, step):
            for x in range(0, width - scale + 1, step):
                windows.append((x, y, x + scale, y + scale))
    
    return windows


# ============================================================================
# 2. æ¨¡å‹ç›¸å…³å‡½æ•°
# ============================================================================

def load_dinov3_model(model_name: str = 'dinov3_vits16', 
                      weights_path: Optional[str] = None, 
                      device: str = 'cuda', 
                      local_repo: Optional[str] = None) -> torch.nn.Module:
    """åŠ è½½DINOv3æ¨¡å‹ç”¨äºç‰¹å¾æå–
    
    Args:
        model_name: æ¨¡å‹åç§°
        weights_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        local_repo: æœ¬åœ°ä»“åº“è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»æœ¬åœ°åŠ è½½
        
    Returns:
        DINOv3æ¨¡å‹å®ä¾‹
    """
    print(f"ğŸ“¦ åŠ è½½DINOv3æ¨¡å‹: {model_name}")
    
    # åŠ è½½æ¨¡å‹ç»“æ„
    model = _load_model_structure(model_name, local_repo)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if weights_path and os.path.exists(weights_path):
        _load_pretrained_weights(model, weights_path)
    
    model = model.to(device)
    model.eval()
    print("âœ“ DINOv3æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model


def _load_model_structure(model_name: str, local_repo: Optional[str]) -> torch.nn.Module:
    """åŠ è½½æ¨¡å‹ç»“æ„"""
    if local_repo and os.path.exists(local_repo):
        print(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°ä»“åº“: {local_repo}")
        model = torch.hub.load(
            repo_or_dir=local_repo,
            model=model_name,
            source='local',
            pretrained=False
        )
    else:
        model = torch.hub.load(
            repo_or_dir='facebookresearch/dinov3',
            model=model_name,
            source='github',
            pretrained=True
        )
    
    return model


def _load_pretrained_weights(model: torch.nn.Module, weights_path: str):
    """åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæƒé‡: {weights_path}")
    state_dict = torch.load(weights_path, map_location='cpu', weights_only=False)
    
    # å¤„ç†ä¸åŒçš„æƒé‡æ ¼å¼
    if 'teacher' in state_dict:
        state_dict = state_dict['teacher']
    elif 'model' in state_dict:
        state_dict = state_dict['model']
    
    # è¿‡æ»¤å¹¶åŠ è½½æƒé‡
    model_keys = set(model.state_dict().keys())
    filtered_dict = {}
    for k, v in state_dict.items():
        new_k = k.replace('backbone.', '').replace('module.', '')
        if new_k in model_keys:
            filtered_dict[new_k] = v
    
    model.load_state_dict(filtered_dict, strict=False)


# ============================================================================
# 3. ç‰¹å¾æå–ç›¸å…³å‡½æ•°
# ============================================================================

def extract_patch_features(model: torch.nn.Module, 
                          image: Image.Image, 
                          bbox: Tuple[int, int, int, int], 
                          patch_size: int = 224, 
                          device: str = 'cuda') -> Optional[np.ndarray]:
    """ä»å›¾åƒçš„å•ä¸ªbboxåŒºåŸŸæå–DINOv3ç‰¹å¾
    
    Args:
        model: DINOv3æ¨¡å‹
        image: PILå›¾åƒ
        bbox: è¾¹ç•Œæ¡† (xmin, ymin, xmax, ymax)
        patch_size: è¾“å…¥patchå¤§å°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        ç‰¹å¾å‘é‡ï¼ˆ1D numpy arrayï¼‰ï¼Œå¦‚æœbboxæ— æ•ˆåˆ™è¿”å›None
    """
    # ç¡®ä¿bboxåœ¨å›¾åƒèŒƒå›´å†…
    clipped_bbox = clip_bbox_to_image(bbox, image.size)
    if clipped_bbox is None:
        return None
    
    # é¢„å¤„ç†å›¾åƒpatch
    patch_tensor = _preprocess_image_patch(image, clipped_bbox, patch_size, device)
    
    # æå–ç‰¹å¾
    with torch.no_grad():
        features = model.forward_features(patch_tensor)
        feat = _extract_cls_token(features)
        feat = feat.cpu().numpy().flatten()
    
    return feat


def _preprocess_image_patch(image: Image.Image, 
                            bbox: Tuple[int, int, int, int],
                            patch_size: int, 
                            device: str) -> torch.Tensor:
    """é¢„å¤„ç†å›¾åƒpatch"""
    xmin, ymin, xmax, ymax = bbox
    
    # è£å‰ªbboxåŒºåŸŸ
    patch = image.crop((xmin, ymin, xmax, ymax))
    
    # è½¬æ¢ä¸ºtensor
    transform = T.Compose([
        T.Resize((patch_size, patch_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    patch_tensor = transform(patch).unsqueeze(0).to(device)
    return patch_tensor


def _extract_cls_token(features) -> torch.Tensor:
    """ä»DINOv3ç‰¹å¾ä¸­æå–CLS token"""
    # ä¼˜å…ˆä½¿ç”¨x_norm_clstoken
    if hasattr(features, 'x_norm_clstoken'):
        return features.x_norm_clstoken
    
    # å°è¯•ä½¿ç”¨tokensçš„ç¬¬ä¸€ä¸ª
    if hasattr(features, 'tokens'):
        return features.tokens[:, 0]
    
    # å¤„ç†å­—å…¸æ ¼å¼çš„è¾“å‡º
    if isinstance(features, dict):
        if 'x_norm_clstoken' in features:
            return features['x_norm_clstoken']
        
        # ä½¿ç”¨patch tokensçš„å¹³å‡å€¼
        feat = features.get('x_norm_patchtokens', features.get('tokens', None))
        if feat is not None and feat.dim() > 2:
            return feat.mean(dim=1)
    
    # å¤„ç†tensorè¾“å‡º
    if features.dim() == 3:
        return features[:, 0]
    
    return features


# ============================================================================
# 4. æ£€æµ‹ç›¸å…³å‡½æ•°
# ============================================================================

def non_max_suppression(boxes: List[Tuple], 
                       scores: List[float], 
                       iou_threshold: float = 0.5) -> List[int]:
    """éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰
    
    Args:
        boxes: è¾¹ç•Œæ¡†åˆ—è¡¨ [(xmin, ymin, xmax, ymax), ...]
        scores: ç½®ä¿¡åº¦åˆ†æ•°åˆ—è¡¨
        iou_threshold: IoUé˜ˆå€¼
        
    Returns:
        ä¿ç•™çš„è¾¹ç•Œæ¡†ç´¢å¼•åˆ—è¡¨
    """
    if len(boxes) == 0:
        return []
    
    boxes = np.array(boxes)
    scores = np.array(scores)
    
    # æå–åæ ‡
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    
    # è®¡ç®—é¢ç§¯
    areas = (x2 - x1) * (y2 - y1)
    
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    order = scores.argsort()[::-1]
    
    keep = []
    while order.size > 0:
        # ä¿ç•™å½“å‰æœ€é«˜åˆ†æ•°çš„æ¡†
        i = order[0]
        keep.append(i)
        
        # è®¡ç®—IoU
        iou = _compute_iou_vectorized(boxes[i], boxes[order[1:]], areas[i], areas[order[1:]])
        
        # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†
        inds = np.where(iou <= iou_threshold)[0]
        order = order[inds + 1]
    
    return keep


def _compute_iou_vectorized(box: np.ndarray, 
                            boxes: np.ndarray, 
                            box_area: float, 
                            boxes_area: np.ndarray) -> np.ndarray:
    """å‘é‡åŒ–è®¡ç®—IoU"""
    xx1 = np.maximum(box[0], boxes[:, 0])
    yy1 = np.maximum(box[1], boxes[:, 1])
    xx2 = np.minimum(box[2], boxes[:, 2])
    yy2 = np.minimum(box[3], boxes[:, 3])
    
    w = np.maximum(0.0, xx2 - xx1)
    h = np.maximum(0.0, yy2 - yy1)
    inter = w * h
    
    iou = inter / (box_area + boxes_area - inter)
    return iou


def apply_nms_per_class(detections: List[Dict], 
                       class_names: List[str],
                       iou_threshold: float = 0.3) -> List[Dict]:
    """å¯¹æ¯ä¸ªç±»åˆ«åˆ†åˆ«åº”ç”¨NMS
    
    Args:
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        iou_threshold: IoUé˜ˆå€¼
        
    Returns:
        NMSåçš„æ£€æµ‹ç»“æœåˆ—è¡¨
    """
    final_results = []
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    for class_idx in range(len(class_names)):
        class_dets = [d for d in detections if d['class_idx'] == class_idx]
        if len(class_dets) == 0:
            continue
        
        boxes = [d['bbox'] for d in class_dets]
        scores = [d['confidence'] for d in class_dets]
        
        # åº”ç”¨NMS
        keep_indices = non_max_suppression(boxes, scores, iou_threshold=iou_threshold)
        
        # æ·»åŠ ä¿ç•™çš„æ£€æµ‹ç»“æœ
        for idx in keep_indices:
            final_results.append({
                'bbox': class_dets[idx]['bbox'],
                'predicted_class': class_names[class_idx],
                'confidence': class_dets[idx]['confidence']
            })
    
    return final_results


# ============================================================================
# 5. åˆ†ç±»å’Œæ£€æµ‹æµç¨‹å‡½æ•°
# ============================================================================

def classify_with_annotations(image_path: Path, 
                             xml_path: Path, 
                             model: torch.nn.Module, 
                             rf_classifier, 
                             config: Dict, 
                             device: str = 'cuda') -> Tuple[Image.Image, List[Dict]]:
    """ä½¿ç”¨å·²æœ‰æ ‡æ³¨æ¡†è¿›è¡Œåˆ†ç±»
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        xml_path: XMLæ ‡æ³¨æ–‡ä»¶è·¯å¾„
        model: DINOv3æ¨¡å‹
        rf_classifier: éšæœºæ£®æ—åˆ†ç±»å™¨
        config: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (å¯è§†åŒ–å›¾åƒ, æ£€æµ‹ç»“æœåˆ—è¡¨)
    """
    # åŠ è½½å›¾åƒå’Œæ ‡æ³¨
    image = Image.open(image_path).convert('RGB')
    annotations = parse_xml_annotation(xml_path)
    
    # å¯¹æ¯ä¸ªæ ‡æ³¨æ¡†è¿›è¡Œåˆ†ç±»
    results = []
    for obj in tqdm(annotations, desc="åˆ†ç±»æ ‡æ³¨æ¡†"):
        bbox = obj['bbox']
        
        # æå–ç‰¹å¾å¹¶é¢„æµ‹
        prediction = _classify_single_bbox(
            model, rf_classifier, image, bbox, 
            config['patch_size'], config['class_names'], device
        )
        
        if prediction is None:
            continue
        
        # æ·»åŠ çœŸå®æ ‡ç­¾ä¿¡æ¯
        results.append({
            'bbox': bbox,
            'predicted_class': prediction['predicted_class'],
            'true_class': obj['class_name'],
            'confidence': prediction['confidence'],
            'correct': prediction['predicted_class'] == obj['class_name']
        })
    
    # å¯è§†åŒ–
    vis_image = visualize_detections(image, results, show_true_labels=True)
    
    return vis_image, results


def _classify_single_bbox(model: torch.nn.Module, 
                         rf_classifier, 
                         image: Image.Image, 
                         bbox: Tuple[int, int, int, int],
                         patch_size: int, 
                         class_names: List[str],
                         device: str) -> Optional[Dict]:
    """å¯¹å•ä¸ªè¾¹ç•Œæ¡†è¿›è¡Œåˆ†ç±»"""
    # æå–ç‰¹å¾
    feat = extract_patch_features(model, image, bbox, patch_size=patch_size, device=device)
    if feat is None:
        return None
    
    # é¢„æµ‹ç±»åˆ«
    pred_label = rf_classifier.predict([feat])[0]
    pred_proba = rf_classifier.predict_proba([feat])[0]
    confidence = pred_proba[pred_label]
    
    return {
        'predicted_class': class_names[pred_label],
        'confidence': confidence
    }


def detect_sliding_window(image_path: Path, 
                         model: torch.nn.Module, 
                         rf_classifier, 
                         config: Dict,
                         device: str = 'cuda', 
                         conf_threshold: float = 0.7,
                         nms_threshold: float = 0.3) -> Tuple[Image.Image, List[Dict]]:
    """ä½¿ç”¨æ»‘åŠ¨çª—å£è¿›è¡Œç›®æ ‡æ£€æµ‹
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        model: DINOv3æ¨¡å‹
        rf_classifier: éšæœºæ£®æ—åˆ†ç±»å™¨
        config: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        nms_threshold: NMSçš„IoUé˜ˆå€¼
        
    Returns:
        (å¯è§†åŒ–å›¾åƒ, æ£€æµ‹ç»“æœåˆ—è¡¨)
    """
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    img_width, img_height = image.size
    
    print(f"å›¾åƒå°ºå¯¸: {img_width} x {img_height}")
    
    # ç”Ÿæˆæ»‘åŠ¨çª—å£
    windows = generate_sliding_windows((img_width, img_height))
    print(f"ç”Ÿæˆ {len(windows)} ä¸ªå€™é€‰çª—å£")
    
    # å¯¹æ¯ä¸ªçª—å£è¿›è¡Œåˆ†ç±»
    detections = _classify_all_windows(
        model, rf_classifier, image, windows,
        config['patch_size'], device, conf_threshold
    )
    
    print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªé«˜ç½®ä¿¡åº¦çª—å£")
    
    # åº”ç”¨NMS
    final_results = apply_nms_per_class(
        detections, config['class_names'], iou_threshold=nms_threshold
    )
    
    print(f"NMSåä¿ç•™ {len(final_results)} ä¸ªæ£€æµ‹æ¡†")
    
    # å¯è§†åŒ–
    vis_image = visualize_detections(image, final_results, show_true_labels=False)
    
    return vis_image, final_results


def _classify_all_windows(model: torch.nn.Module, 
                         rf_classifier, 
                         image: Image.Image, 
                         windows: List[Tuple[int, int, int, int]],
                         patch_size: int, 
                         device: str,
                         conf_threshold: float) -> List[Dict]:
    """å¯¹æ‰€æœ‰çª—å£è¿›è¡Œåˆ†ç±»"""
    detections = []
    
    for bbox in tqdm(windows, desc="å¤„ç†çª—å£"):
        # æå–ç‰¹å¾
        feat = extract_patch_features(model, image, bbox, patch_size=patch_size, device=device)
        if feat is None:
            continue
        
        # é¢„æµ‹
        pred_proba = rf_classifier.predict_proba([feat])[0]
        pred_label = pred_proba.argmax()
        confidence = pred_proba[pred_label]
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦
        if confidence > conf_threshold:
            detections.append({
                'bbox': bbox,
                'class_idx': pred_label,
                'confidence': confidence
            })
    
    return detections


# ============================================================================
# 6. å¯è§†åŒ–ç›¸å…³å‡½æ•°
# ============================================================================

def visualize_detections(image: Image.Image, 
                         results: List[Dict], 
                         show_true_labels: bool = False) -> Image.Image:
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ
    
    Args:
        image: PILå›¾åƒ
        results: æ£€æµ‹ç»“æœåˆ—è¡¨
        show_true_labels: æ˜¯å¦æ˜¾ç¤ºçœŸå®æ ‡ç­¾ï¼ˆç”¨äºè¯„ä¼°æ¨¡å¼ï¼‰
        
    Returns:
        å¯è§†åŒ–åçš„å›¾åƒ
    """
    vis_image = image.copy()
    draw = ImageDraw.Draw(vis_image)
    font = _load_font()
    
    for det in results:
        _draw_single_detection(draw, det, font, show_true_labels)
    
    return vis_image


def _load_font() -> ImageFont.FreeTypeFont:
    """åŠ è½½å­—ä½“"""
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
    except:
        font = ImageFont.load_default()
    
    return font


def _draw_single_detection(draw: ImageDraw.Draw, 
                          detection: Dict, 
                          font: ImageFont.FreeTypeFont,
                          show_true_labels: bool):
    """ç»˜åˆ¶å•ä¸ªæ£€æµ‹ç»“æœ"""
    bbox = detection['bbox']
    pred_class = detection['predicted_class']
    confidence = detection['confidence']
    
    # è·å–é¢œè‰²
    color = COLORS.get(pred_class, (255, 255, 255))
    
    # ç”»è¾¹ç•Œæ¡†
    draw.rectangle(bbox, outline=color, width=3)
    
    # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
    label = _format_label(detection, show_true_labels)
    
    # ç”»æ ‡ç­¾èƒŒæ™¯å’Œæ–‡æœ¬
    text_bbox = draw.textbbox((bbox[0], bbox[1] - 20), label, font=font)
    draw.rectangle(text_bbox, fill=color)
    draw.text((bbox[0], bbox[1] - 20), label, fill=(255, 255, 255), font=font)


def _format_label(detection: Dict, show_true_labels: bool) -> str:
    """æ ¼å¼åŒ–æ ‡ç­¾æ–‡æœ¬"""
    pred_class = detection['predicted_class']
    confidence = detection['confidence']
    
    if show_true_labels and 'true_class' in detection:
        true_class = detection['true_class']
        correct = detection['correct']
        status = "âœ“" if correct else "âœ—"
        return f"{status} {pred_class} (GT: {true_class}) {confidence:.2f}"
    else:
        return f"{pred_class} {confidence:.2f}"


# ============================================================================
# 7. ç»Ÿè®¡å’Œè¯„ä¼°ç›¸å…³å‡½æ•°
# ============================================================================

def compute_classification_stats(results: List[Dict]) -> Dict:
    """è®¡ç®—åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        results: åŒ…å«true_classå’Œcorrectå­—æ®µçš„ç»“æœåˆ—è¡¨
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    total = len(results)
    correct = sum(1 for r in results if r['correct'])
    accuracy = correct / total if total > 0 else 0
    
    return {
        'total': total,
        'correct': correct,
        'accuracy': accuracy
    }


def compute_per_class_stats(results: List[Dict], class_names: List[str]) -> Dict[str, Dict]:
    """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        results: åŒ…å«true_classå’Œcorrectå­—æ®µçš„ç»“æœåˆ—è¡¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        
    Returns:
        æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
    """
    class_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for r in results:
        true_class = r['true_class']
        class_stats[true_class]['total'] += 1
        if r['correct']:
            class_stats[true_class]['correct'] += 1
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    for class_name in class_names:
        if class_name in class_stats:
            stats = class_stats[class_name]
            stats['accuracy'] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
    
    return dict(class_stats)


def compute_detection_stats(results: List[Dict], class_names: List[str]) -> Dict[str, int]:
    """è®¡ç®—æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        results: æ£€æµ‹ç»“æœåˆ—è¡¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        
    Returns:
        æ¯ä¸ªç±»åˆ«çš„æ£€æµ‹æ•°é‡
    """
    class_counts = Counter([r['predicted_class'] for r in results])
    
    # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½æœ‰è®¡æ•°
    stats = {class_name: class_counts.get(class_name, 0) for class_name in class_names}
    
    return stats


def print_classification_results(stats: Dict, per_class_stats: Dict, class_names: List[str]):
    """æ‰“å°åˆ†ç±»ç»“æœç»Ÿè®¡"""
    print(f"\nç»“æœç»Ÿè®¡:")
    print(f"  æ€»æ•°: {stats['total']}")
    print(f"  æ­£ç¡®: {stats['correct']}")
    print(f"  å‡†ç¡®ç‡: {stats['accuracy']:.2%}")
    
    print(f"\nå„ç±»åˆ«å‡†ç¡®ç‡:")
    for class_name in class_names:
        if class_name in per_class_stats:
            class_stat = per_class_stats[class_name]
            print(f"  {class_name}: {class_stat['correct']}/{class_stat['total']} = {class_stat['accuracy']:.2%}")


def print_detection_results(stats: Dict[str, int]):
    """æ‰“å°æ£€æµ‹ç»“æœç»Ÿè®¡"""
    print(f"\næ£€æµ‹ç»Ÿè®¡:")
    for class_name, count in stats.items():
        print(f"  {class_name}: {count}")


# ============================================================================
# 8. æ¨¡å‹åŠ è½½å’Œé…ç½®ç›¸å…³å‡½æ•°
# ============================================================================

def load_model_and_config(model_dir: Path, 
                         device: str,
                         local_repo: Optional[str] = None) -> Tuple[torch.nn.Module, object, Dict]:
    """åŠ è½½é…ç½®ã€DINOv3æ¨¡å‹å’Œéšæœºæ£®æ—åˆ†ç±»å™¨
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•
        device: è®¡ç®—è®¾å¤‡
        local_repo: æœ¬åœ°ä»“åº“è·¯å¾„
        
    Returns:
        (dinov3_model, rf_classifier, config)
    """
    # åŠ è½½é…ç½®
    config_path = model_dir / 'config.pkl'
    print(f"ğŸ“¦ åŠ è½½é…ç½®: {config_path}")
    with open(config_path, 'rb') as f:
        config = pickle.load(f)
    
    # åŠ è½½éšæœºæ£®æ—æ¨¡å‹
    rf_path = model_dir / 'rf_classifier.pkl'
    print(f"ğŸ“¦ åŠ è½½éšæœºæ£®æ—æ¨¡å‹: {rf_path}")
    rf_classifier = joblib.load(rf_path)
    
    # åŠ è½½DINOv3æ¨¡å‹
    dinov3_model = load_dinov3_model(
        model_name=config['dinov3_model'],
        weights_path=config.get('dinov3_weights'),
        device=device,
        local_repo=local_repo
    )
    
    return dinov3_model, rf_classifier, config


def validate_paths(image_path: Path, xml_path: Optional[Path] = None) -> bool:
    """éªŒè¯è¾“å…¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        xml_path: XMLæ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æ˜¯å¦æ‰€æœ‰è·¯å¾„éƒ½æœ‰æ•ˆ
    """
    if not image_path.exists():
        print(f"é”™è¯¯: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        return False
    
    if xml_path and not xml_path.exists():
        print(f"é”™è¯¯: XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_path}")
        return False
    
    return True


def determine_output_path(output_arg: Optional[str], image_path: Path) -> Path:
    """ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Args:
        output_arg: å‘½ä»¤è¡ŒæŒ‡å®šçš„è¾“å‡ºè·¯å¾„
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if output_arg:
        return Path(output_arg)
    else:
        output_dir = Path('output/bccd_rf_detections')
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir / f"{image_path.stem}_detection.png"


# ============================================================================
# 9. ä¸»ç¨‹åºç›¸å…³å‡½æ•°
# ============================================================================

def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='è¡€ç»†èƒåˆ†ç±»æ¨ç† - DINOv3 + éšæœºæ£®æ—')
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument('--image', type=str, required=True,
                       help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--xml', type=str, default=None,
                       help='XMLæ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œåˆ™ä½¿ç”¨æ ‡æ³¨æ¡†è¿›è¡Œåˆ†ç±»ï¼›å¦åˆ™ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼‰')
    parser.add_argument('--model_dir', type=str, default='output/bccd_rf_model',
                       help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºå›¾åƒè·¯å¾„')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--conf_threshold', type=float, default=0.7,
                       help='ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä»…ç”¨äºæ»‘åŠ¨çª—å£æ¨¡å¼ï¼‰')
    parser.add_argument('--nms_threshold', type=float, default=0.3,
                       help='NMSçš„IoUé˜ˆå€¼ï¼ˆä»…ç”¨äºæ»‘åŠ¨çª—å£æ¨¡å¼ï¼‰')
    parser.add_argument('--local_repo', type=str, default='/data/william/Workspace/dinov3',
                       help='æœ¬åœ°DINOv3ä»“åº“è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda æˆ– cpu)')
    
    return parser.parse_args()


def setup_device(device: str) -> str:
    """è®¾ç½®å¹¶éªŒè¯è®¡ç®—è®¾å¤‡
    
    Args:
        device: æœŸæœ›çš„è®¾å¤‡
        
    Returns:
        å®é™…ä½¿ç”¨çš„è®¾å¤‡
    """
    if device == 'cuda' and not torch.cuda.is_available():
        print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return 'cpu'
    return device


def print_header():
    """æ‰“å°ç¨‹åºæ ‡é¢˜"""
    print("="*80)
    print("è¡€ç»†èƒåˆ†ç±»æ¨ç† - DINOv3ç‰¹å¾ + éšæœºæ£®æ—åˆ†ç±»å™¨")
    print("="*80)


def run_annotation_mode(image_path: Path, 
                       xml_path: Path,
                       dinov3_model: torch.nn.Module,
                       rf_classifier,
                       config: Dict,
                       device: str) -> Tuple[Image.Image, List[Dict]]:
    """è¿è¡Œæ ‡æ³¨æ¡†åˆ†ç±»æ¨¡å¼
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        xml_path: XMLæ ‡æ³¨è·¯å¾„
        dinov3_model: DINOv3æ¨¡å‹
        rf_classifier: éšæœºæ£®æ—åˆ†ç±»å™¨
        config: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (å¯è§†åŒ–å›¾åƒ, ç»“æœåˆ—è¡¨)
    """
    print(f"\næ¨¡å¼: ä½¿ç”¨æ ‡æ³¨æ¡†è¿›è¡Œåˆ†ç±»")
    print(f"å›¾åƒ: {image_path}")
    print(f"æ ‡æ³¨: {xml_path}")
    
    vis_image, results = classify_with_annotations(
        image_path, xml_path, dinov3_model, rf_classifier, config, device
    )
    
    # è®¡ç®—å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = compute_classification_stats(results)
    per_class_stats = compute_per_class_stats(results, config['class_names'])
    print_classification_results(stats, per_class_stats, config['class_names'])
    
    return vis_image, results


def run_detection_mode(image_path: Path,
                      dinov3_model: torch.nn.Module,
                      rf_classifier,
                      config: Dict,
                      device: str,
                      conf_threshold: float,
                      nms_threshold: float) -> Tuple[Image.Image, List[Dict]]:
    """è¿è¡Œæ»‘åŠ¨çª—å£æ£€æµ‹æ¨¡å¼
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        dinov3_model: DINOv3æ¨¡å‹
        rf_classifier: éšæœºæ£®æ—åˆ†ç±»å™¨
        config: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        nms_threshold: NMSé˜ˆå€¼
        
    Returns:
        (å¯è§†åŒ–å›¾åƒ, ç»“æœåˆ—è¡¨)
    """
    print(f"\næ¨¡å¼: æ»‘åŠ¨çª—å£æ£€æµ‹")
    print(f"å›¾åƒ: {image_path}")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    print(f"NMSé˜ˆå€¼: {nms_threshold}")
    
    vis_image, results = detect_sliding_window(
        image_path, dinov3_model, rf_classifier, config,
        device, conf_threshold, nms_threshold
    )
    
    # è®¡ç®—å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = compute_detection_stats(results, config['class_names'])
    print_detection_results(stats)
    
    return vis_image, results


def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•´ä¸ªæ¨ç†æµç¨‹"""
    # 1. è§£æå‚æ•°å’Œè®¾ç½®
    args = parse_arguments()
    args.device = setup_device(args.device)
    print_header()
    
    # 2. åŠ è½½æ¨¡å‹å’Œé…ç½®
    model_dir = Path(args.model_dir)
    dinov3_model, rf_classifier, config = load_model_and_config(
        model_dir, args.device, args.local_repo
    )
    
    # 3. éªŒè¯è¾“å…¥è·¯å¾„
    image_path = Path(args.image)
    xml_path = Path(args.xml) if args.xml else None
    
    if not validate_paths(image_path, xml_path):
        return
    
    # 4. æ ¹æ®æ¨¡å¼è¿è¡Œæ¨ç†
    if xml_path:
        # æ ‡æ³¨æ¡†åˆ†ç±»æ¨¡å¼
        vis_image, results = run_annotation_mode(
            image_path, xml_path, dinov3_model, rf_classifier, config, args.device
        )
    else:
        # æ»‘åŠ¨çª—å£æ£€æµ‹æ¨¡å¼
        vis_image, results = run_detection_mode(
            image_path, dinov3_model, rf_classifier, config, 
            args.device, args.conf_threshold, args.nms_threshold
        )
    
    # 5. ä¿å­˜ç»“æœ
    output_path = determine_output_path(args.output, image_path)
    vis_image.save(output_path)
    print(f"\nâœ“ ç»“æœä¿å­˜åˆ°: {output_path}")
    
    print("="*80)
    print("æ¨ç†å®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main()
