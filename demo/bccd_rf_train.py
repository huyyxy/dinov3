#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This software may be used and distributed in accordance with
# the terms of the DINOv3 License Agreement.

"""
è¡€ç»†èƒåˆ†ç±»è®­ç»ƒè„šæœ¬ - ä½¿ç”¨DINOv3ç‰¹å¾ + éšæœºæ£®æ—åˆ†ç±»å™¨
è¿™ç§æ–¹æ³•æ›´ç®€å•ç›´æ¥ï¼Œé€šå¸¸åœ¨å°æ•°æ®é›†ä¸Šæ•ˆæœæ›´å¥½
"""

import argparse
import os
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Tuple
import pickle
import time

import torch
import torch.nn.functional as F
from torchvision import transforms as T
from PIL import Image
import numpy as np
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))


# ç±»åˆ«æ˜ å°„
CLASS_NAMES = ['Platelets', 'RBC', 'WBC']
CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}


def parse_xml_annotation(xml_file: Path) -> List[Dict]:
    """è§£æPASCAL VOCæ ¼å¼çš„XMLæ ‡æ³¨æ–‡ä»¶
    
    Args:
        xml_file: XMLæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ ‡æ³¨å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«class_nameå’Œbbox
    """
    tree = ET.parse(xml_file)
    root = tree.getroot()
    
    objects = []
    for obj in root.findall('object'):
        class_name = obj.find('name').text
        bbox = obj.find('bndbox')
        xmin = int(bbox.find('xmin').text)
        ymin = int(bbox.find('ymin').text)
        xmax = int(bbox.find('xmax').text)
        ymax = int(bbox.find('ymax').text)
        
        objects.append({
            'class_name': class_name,
            'bbox': (xmin, ymin, xmax, ymax)
        })
    
    return objects


def load_dinov3_model(model_name: str = 'dinov3_vits16', weights_path: str = None, 
                      device='cuda', local_repo: str = None):
    """åŠ è½½DINOv3æ¨¡å‹ç”¨äºç‰¹å¾æå–
    
    Args:
        model_name: æ¨¡å‹åç§°
        weights_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        device: è®¾å¤‡
        local_repo: æœ¬åœ°ä»“åº“è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»æœ¬åœ°åŠ è½½
        
    Returns:
        DINOv3æ¨¡å‹
    """
    print(f"ğŸ“¦ åŠ è½½DINOv3æ¨¡å‹: {model_name}")
    
    # å¦‚æœæä¾›äº†æœ¬åœ°ä»“åº“è·¯å¾„ï¼Œä»æœ¬åœ°åŠ è½½
    if local_repo and os.path.exists(local_repo):
        print(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°ä»“åº“: {local_repo}")
        model = torch.hub.load(
            repo_or_dir=local_repo,
            model=model_name,
            source='local',
            pretrained=False  # ç¨åæ‰‹åŠ¨åŠ è½½æƒé‡
        )
    else:
        # ä»GitHubåŠ è½½
        model = torch.hub.load(
            repo_or_dir='facebookresearch/dinov3',
            model=model_name,
            source='github',
            pretrained=True  # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼
        )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if weights_path and os.path.exists(weights_path):
        print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæƒé‡: {weights_path}")
        state_dict = torch.load(weights_path, map_location='cpu', weights_only=False)
        
        if 'teacher' in state_dict:
            state_dict = state_dict['teacher']
        elif 'model' in state_dict:
            state_dict = state_dict['model']
        
        model_keys = set(model.state_dict().keys())
        filtered_dict = {}
        for k, v in state_dict.items():
            new_k = k.replace('backbone.', '').replace('module.', '')
            if new_k in model_keys:
                filtered_dict[new_k] = v
        
        model.load_state_dict(filtered_dict, strict=False)
    
    model = model.to(device)
    model.eval()
    print("âœ“ DINOv3æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model


def extract_patch_features(model, image: Image.Image, bbox: Tuple[int, int, int, int], 
                          patch_size: int = 224, device='cuda') -> np.ndarray:
    """ä»å›¾åƒä¸­æå–å•ä¸ªbboxçš„ç‰¹å¾
    
    Args:
        model: DINOv3æ¨¡å‹
        image: PILå›¾åƒ
        bbox: è¾¹ç•Œæ¡† (xmin, ymin, xmax, ymax)
        patch_size: è¾“å…¥patchå¤§å°
        device: è®¾å¤‡
        
    Returns:
        ç‰¹å¾å‘é‡
    """
    xmin, ymin, xmax, ymax = bbox
    
    # è£å‰ªå¹¶resize bboxåŒºåŸŸ
    patch = image.crop((xmin, ymin, xmax, ymax))
    
    # è½¬æ¢ä¸ºtensor
    transform = T.Compose([
        T.Resize((patch_size, patch_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    patch_tensor = transform(patch).unsqueeze(0).to(device)
    
    # æå–ç‰¹å¾
    with torch.no_grad():
        features = model.forward_features(patch_tensor)
        
        # ä½¿ç”¨CLS tokençš„ç‰¹å¾
        if hasattr(features, 'x_norm_clstoken'):
            feat = features.x_norm_clstoken
        elif hasattr(features, 'tokens'):
            feat = features.tokens[:, 0]  # CLS token
        else:
            # å¦‚æœæ˜¯å­—å…¸å½¢å¼çš„è¾“å‡º
            if isinstance(features, dict):
                if 'x_norm_clstoken' in features:
                    feat = features['x_norm_clstoken']
                else:
                    # å°è¯•ä½¿ç”¨å®Œæ•´çš„patch tokensçš„å¹³å‡å€¼
                    feat = features.get('x_norm_patchtokens', features.get('tokens', None))
                    if feat is not None and feat.dim() > 2:
                        feat = feat.mean(dim=1)
            else:
                # å‡è®¾æ˜¯å•ä¸ªtensorï¼Œå–ç¬¬ä¸€ä¸ªtoken
                if features.dim() == 3:
                    feat = features[:, 0]
                else:
                    feat = features
        
        # è½¬æ¢ä¸ºnumpy
        feat = feat.cpu().numpy().flatten()
    
    return feat


def extract_dataset_features(model, data_dir: Path, annotations_dir: Path, 
                             split: str = 'train', device='cuda', 
                             max_samples: int = None) -> Tuple[np.ndarray, np.ndarray]:
    """ä»æ•°æ®é›†ä¸­æå–æ‰€æœ‰bboxçš„ç‰¹å¾
    
    Args:
        model: DINOv3æ¨¡å‹
        data_dir: å›¾åƒç›®å½•
        annotations_dir: æ ‡æ³¨ç›®å½•
        split: æ•°æ®é›†åˆ’åˆ† ('train' æˆ– 'test')
        device: è®¾å¤‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        
    Returns:
        (features, labels) - ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾æ•°ç»„
    """
    print(f"\nğŸ“Š æå–{split}é›†ç‰¹å¾...")
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = sorted(list(data_dir.glob('*.jpg')))
    if max_samples:
        image_files = image_files[:max_samples]
    
    features_list = []
    labels_list = []
    
    for img_file in tqdm(image_files, desc=f"å¤„ç†{split}é›†"):
        # æŸ¥æ‰¾å¯¹åº”çš„XMLæ ‡æ³¨æ–‡ä»¶
        xml_file = annotations_dir / f"{img_file.stem}.xml"
        if not xml_file.exists():
            continue
        
        # è§£ææ ‡æ³¨
        annotations = parse_xml_annotation(xml_file)
        if not annotations:
            continue
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_file).convert('RGB')
        
        # æå–æ¯ä¸ªbboxçš„ç‰¹å¾
        for obj in annotations:
            class_name = obj['class_name']
            if class_name not in CLASS_TO_IDX:
                continue
            
            bbox = obj['bbox']
            
            try:
                # æå–ç‰¹å¾
                feat = extract_patch_features(model, image, bbox, device=device)
                features_list.append(feat)
                labels_list.append(CLASS_TO_IDX[class_name])
            except Exception as e:
                print(f"è­¦å‘Š: æå–ç‰¹å¾å¤±è´¥ {img_file.name} - {class_name}: {e}")
                continue
    
    features = np.array(features_list)
    labels = np.array(labels_list)
    
    print(f"âœ“ æå–å®Œæˆ: {len(features)} ä¸ªæ ·æœ¬")
    print(f"  ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(labels, return_counts=True)))}")
    
    return features, labels


def balance_dataset(X: np.ndarray, y: np.ndarray, 
                   method: str = 'undersample', 
                   random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:
    """å¹³è¡¡æ•°æ®é›†
    
    Args:
        X: ç‰¹å¾
        y: æ ‡ç­¾
        method: å¹³è¡¡æ–¹æ³• ('undersample' æˆ– 'oversample')
        random_state: éšæœºç§å­
        
    Returns:
        å¹³è¡¡åçš„ç‰¹å¾å’Œæ ‡ç­¾
    """
    from collections import Counter
    
    # éªŒè¯è¾“å…¥æ•°æ®
    if len(X) == 0 or len(y) == 0:
        raise ValueError(f"âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼X shape: {X.shape}, y shape: {y.shape}")
    
    if len(X) != len(y):
        raise ValueError(f"âŒ X å’Œ y çš„é•¿åº¦ä¸åŒ¹é…ï¼X: {len(X)}, y: {len(y)}")
    
    np.random.seed(random_state)
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    unique, counts = np.unique(y, return_counts=True)
    
    if len(unique) == 0:
        raise ValueError(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç±»åˆ«ï¼è¯·æ£€æŸ¥æ ‡ç­¾æ•°æ®")
    
    class_counts = dict(zip(unique, counts))
    
    print(f"\nâš–ï¸  å¹³è¡¡æ•°æ®é›† (æ–¹æ³•: {method})")
    print(f"  åŸå§‹åˆ†å¸ƒ: {class_counts}")
    
    if method == 'undersample':
        # ä¸‹é‡‡æ ·åˆ°æœ€å°ç±»åˆ«çš„æ•°é‡
        min_samples = min(counts)
        
        balanced_X = []
        balanced_y = []
        
        for class_idx in unique:
            class_mask = (y == class_idx)
            class_X = X[class_mask]
            class_y = y[class_mask]
            
            # éšæœºé‡‡æ ·
            indices = np.random.choice(len(class_X), min_samples, replace=False)
            balanced_X.append(class_X[indices])
            balanced_y.append(class_y[indices])
        
        X_balanced = np.vstack(balanced_X)
        y_balanced = np.hstack(balanced_y)
        
    elif method == 'oversample':
        # ä¸Šé‡‡æ ·åˆ°æœ€å¤§ç±»åˆ«çš„æ•°é‡
        max_samples = max(counts)
        
        balanced_X = []
        balanced_y = []
        
        for class_idx in unique:
            class_mask = (y == class_idx)
            class_X = X[class_mask]
            class_y = y[class_mask]
            
            # éšæœºé‡‡æ ·ï¼ˆå…è®¸é‡å¤ï¼‰
            indices = np.random.choice(len(class_X), max_samples, replace=True)
            balanced_X.append(class_X[indices])
            balanced_y.append(class_y[indices])
        
        X_balanced = np.vstack(balanced_X)
        y_balanced = np.hstack(balanced_y)
    
    else:
        raise ValueError(f"Unknown method: {method}")
    
    # æ‰“ä¹±æ•°æ®
    shuffle_indices = np.random.permutation(len(X_balanced))
    X_balanced = X_balanced[shuffle_indices]
    y_balanced = y_balanced[shuffle_indices]
    
    balanced_counts = dict(zip(*np.unique(y_balanced, return_counts=True)))
    print(f"  å¹³è¡¡ååˆ†å¸ƒ: {balanced_counts}")
    print(f"  æ€»æ ·æœ¬: {len(X_balanced)} (åŸå§‹: {len(X)})")
    
    return X_balanced, y_balanced


def train_random_forest(X_train: np.ndarray, y_train: np.ndarray,
                       X_val: np.ndarray = None, y_val: np.ndarray = None,
                       n_estimators: int = 200, max_depth: int = 30,
                       random_state: int = 42,
                       balance_method: str = None) -> RandomForestClassifier:
    """è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨
    
    Args:
        X_train: è®­ç»ƒç‰¹å¾
        y_train: è®­ç»ƒæ ‡ç­¾
        X_val: éªŒè¯ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        y_val: éªŒè¯æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        n_estimators: æ ‘çš„æ•°é‡
        max_depth: æœ€å¤§æ·±åº¦
        random_state: éšæœºç§å­
        balance_method: æ•°æ®å¹³è¡¡æ–¹æ³• ('undersample', 'oversample', None)
        
    Returns:
        è®­ç»ƒå¥½çš„éšæœºæ£®æ—åˆ†ç±»å™¨
    """
    # å¹³è¡¡æ•°æ®é›†
    if balance_method:
        X_train, y_train = balance_dataset(X_train, y_train, 
                                          method=balance_method, 
                                          random_state=random_state)
    
    print(f"\nğŸŒ² è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨...")
    print(f"  å‚æ•°: n_estimators={n_estimators}, max_depth={max_depth}")
    
    start_time = time.time()
    
    # åˆ›å»ºå¹¶è®­ç»ƒéšæœºæ£®æ—
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        verbose=1,
        class_weight='balanced',  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        min_samples_split=5,  # å‡å°‘è¿‡æ‹Ÿåˆ
        min_samples_leaf=2
    )
    
    clf.fit(X_train, y_train)
    
    train_time = time.time() - start_time
    print(f"âœ“ è®­ç»ƒå®Œæˆ (ç”¨æ—¶: {train_time:.2f}ç§’)")
    
    # è¯„ä¼°è®­ç»ƒé›†
    train_acc = clf.score(X_train, y_train)
    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
    
    y_train_pred = clf.predict(X_train)
    print("\nè®­ç»ƒé›†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_train, y_train_pred, target_names=CLASS_NAMES))
    
    # å¦‚æœæä¾›äº†éªŒè¯é›†ï¼Œè¯„ä¼°éªŒè¯é›†
    if X_val is not None and y_val is not None:
        val_acc = clf.score(X_val, y_val)
        print(f"\néªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
        
        y_val_pred = clf.predict(X_val)
        print("\néªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_val, y_val_pred, target_names=CLASS_NAMES))
        
        print("\néªŒè¯é›†æ··æ·†çŸ©é˜µ:")
        print(confusion_matrix(y_val, y_val_pred))
    
    # ç‰¹å¾é‡è¦æ€§
    print("\nç‰¹å¾é‡è¦æ€§ç»Ÿè®¡:")
    print(f"  å¹³å‡é‡è¦æ€§: {clf.feature_importances_.mean():.6f}")
    print(f"  æœ€å¤§é‡è¦æ€§: {clf.feature_importances_.max():.6f}")
    print(f"  æœ€å°é‡è¦æ€§: {clf.feature_importances_.min():.6f}")
    
    return clf


def main():
    parser = argparse.ArgumentParser(description='è¡€ç»†èƒåˆ†ç±»è®­ç»ƒ - DINOv3 + éšæœºæ£®æ—')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--data_root', type=str, default='../BCCD_Dataset',
                       help='BCCDæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='output/bccd_rf_model',
                       help='è¾“å‡ºç›®å½•')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--dinov3_model', type=str, default='dinov3_vits16',
                       choices=['dinov3_vits16', 'dinov3_vitb16', 'dinov3_vitl16', 'dinov3_vitg14',
                               'dinov3_vith16plus', 'dinov3_vit7b16', 'dinov3_vits16plus', 'dinov3_vitl16plus'],
                       help='DINOv3æ¨¡å‹ç±»å‹')
    parser.add_argument('--dinov3_weights', type=str, default=None,
                       help='DINOv3é¢„è®­ç»ƒæƒé‡è·¯å¾„')
    parser.add_argument('--patch_size', type=int, default=224,
                       help='è¾“å…¥patchå¤§å°')
    
    # éšæœºæ£®æ—å‚æ•°
    parser.add_argument('--n_estimators', type=int, default=200,
                       help='éšæœºæ£®æ—æ ‘çš„æ•°é‡')
    parser.add_argument('--max_depth', type=int, default=30,
                       help='éšæœºæ£®æ—æœ€å¤§æ·±åº¦')
    parser.add_argument('--balance_method', type=str, default='oversample',
                       choices=['undersample', 'oversample', 'none'],
                       help='æ•°æ®å¹³è¡¡æ–¹æ³•ï¼šundersample(ä¸‹é‡‡æ ·), oversample(ä¸Šé‡‡æ ·), none(ä¸å¹³è¡¡)')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--local_repo', type=str, default='/data/william/Workspace/dinov3',
                       help='æœ¬åœ°DINOv3ä»“åº“è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda æˆ– cpu)')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--random_seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.random_seed)
    torch.manual_seed(args.random_seed)
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = 'cpu'
    
    print("="*80)
    print("è¡€ç»†èƒåˆ†ç±»è®­ç»ƒ - DINOv3ç‰¹å¾ + éšæœºæ£®æ—åˆ†ç±»å™¨")
    print("="*80)
    print(f"æ•°æ®é›†: {args.data_root}")
    print(f"DINOv3æ¨¡å‹: {args.dinov3_model}")
    print(f"éšæœºæ£®æ—å‚æ•°: n_estimators={args.n_estimators}, max_depth={args.max_depth}")
    print(f"æ•°æ®å¹³è¡¡æ–¹æ³•: {args.balance_method}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è®¾å¤‡: {args.device}")
    print("="*80)
    
    # æ•°æ®è·¯å¾„
    data_root = Path(args.data_root)
    train_img_dir = data_root / 'BCCD' / 'JPEGImages'
    train_ann_dir = data_root / 'BCCD' / 'Annotations'
    test_img_dir = data_root / 'BCCD_Dataset' / 'BCCD' / 'JPEGImages'
    test_ann_dir = data_root / 'BCCD_Dataset' / 'BCCD' / 'Annotations'
    
    # å¦‚æœæµ‹è¯•é›†è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨è®­ç»ƒé›†è·¯å¾„
    if not test_img_dir.exists():
        test_img_dir = train_img_dir
        test_ann_dir = train_ann_dir
        print("æ³¨æ„: ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºéªŒè¯é›†")
    
    # åŠ è½½DINOv3æ¨¡å‹
    dinov3_model = load_dinov3_model(
        model_name=args.dinov3_model,
        weights_path=args.dinov3_weights,
        device=args.device,
        local_repo=args.local_repo
    )
    
    # æå–è®­ç»ƒé›†ç‰¹å¾
    X_train, y_train = extract_dataset_features(
        dinov3_model, 
        train_img_dir, 
        train_ann_dir,
        split='train',
        device=args.device,
        max_samples=args.max_samples
    )
    
    # éªŒè¯è®­ç»ƒæ•°æ®
    if X_train is None or y_train is None or len(X_train) == 0 or len(y_train) == 0:
        print(f"\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æå–åˆ°è®­ç»ƒæ•°æ®ï¼")
        print(f"  è¯·æ£€æŸ¥ï¼š")
        print(f"  1. è®­ç»ƒå›¾ç‰‡ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å›¾ç‰‡: {train_img_dir}")
        print(f"  2. æ ‡æ³¨ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ ‡æ³¨æ–‡ä»¶: {train_ann_dir}")
        print(f"  3. å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶çš„æ ¼å¼æ˜¯å¦æ­£ç¡®")
        return
    
    print(f"\nâœ… æˆåŠŸæå–è®­ç»ƒç‰¹å¾: {len(X_train)} ä¸ªæ ·æœ¬")
    
    # æå–éªŒè¯é›†ç‰¹å¾
    X_val, y_val = None, None
    if test_img_dir != train_img_dir:
        X_val, y_val = extract_dataset_features(
            dinov3_model,
            test_img_dir,
            test_ann_dir,
            split='test',
            device=args.device,
            max_samples=args.max_samples
        )
        if X_val is not None and len(X_val) > 0:
            print(f"âœ… æˆåŠŸæå–éªŒè¯ç‰¹å¾: {len(X_val)} ä¸ªæ ·æœ¬")
    
    # åº”ç”¨æ•°æ®å¹³è¡¡ï¼ˆåœ¨è®­ç»ƒä¹‹å‰ï¼‰
    balance_method = None if args.balance_method == 'none' else args.balance_method
    if balance_method:
        print(f"\nâš–ï¸  åº”ç”¨æ•°æ®å¹³è¡¡æ–¹æ³•: {balance_method}")
        X_train, y_train = balance_dataset(
            X_train, y_train,
            method=balance_method,
            random_state=args.random_seed
        )
    
    # ä¿å­˜ç‰¹å¾ï¼ˆå¹³è¡¡åçš„æ•°æ®ï¼‰
    features_path = output_dir / 'features.pkl'
    print(f"\nğŸ’¾ ä¿å­˜ç‰¹å¾åˆ°: {features_path}")
    with open(features_path, 'wb') as f:
        pickle.dump({
            'X_train': X_train,
            'y_train': y_train,
            'X_val': X_val,
            'y_val': y_val,
            'class_names': CLASS_NAMES,
            'class_to_idx': CLASS_TO_IDX,
            'balance_method': args.balance_method
        }, f)
    
    # è®­ç»ƒéšæœºæ£®æ—ï¼ˆä½¿ç”¨å·²å¹³è¡¡çš„æ•°æ®ï¼Œä¸å†åœ¨å‡½æ•°å†…éƒ¨å¹³è¡¡ï¼‰
    rf_classifier = train_random_forest(
        X_train, y_train,
        X_val, y_val,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.random_seed,
        balance_method=None  # æ•°æ®å·²ç»å¹³è¡¡ï¼Œä¸éœ€è¦å†å¹³è¡¡
    )
    
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / 'rf_classifier.pkl'
    print(f"\nğŸ’¾ ä¿å­˜éšæœºæ£®æ—æ¨¡å‹åˆ°: {model_path}")
    joblib.dump(rf_classifier, model_path)
    
    # ä¿å­˜é…ç½®
    config_path = output_dir / 'config.pkl'
    print(f"ğŸ’¾ ä¿å­˜é…ç½®åˆ°: {config_path}")
    config = {
        'dinov3_model': args.dinov3_model,
        'dinov3_weights': args.dinov3_weights,
        'patch_size': args.patch_size,
        'n_estimators': args.n_estimators,
        'max_depth': args.max_depth,
        'balance_method': args.balance_method,  # ä¿å­˜å¹³è¡¡æ–¹æ³•
        'class_names': CLASS_NAMES,
        'class_to_idx': CLASS_TO_IDX
    }
    with open(config_path, 'wb') as f:
        pickle.dump(config, f)
    
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    print("="*80)


if __name__ == '__main__':
    main()

