ğŸ†• [2025-09-17] :fire: DINOv3 éª¨å¹²ç½‘ç»œç°åœ¨å—åˆ° [PyTorch Image Models / timm](https://github.com/huggingface/pytorch-image-models/) åº“æ”¯æŒï¼Œä»ç‰ˆæœ¬ [1.0.20](https://github.com/huggingface/pytorch-image-models/releases/tag/v1.0.20) å¼€å§‹

[2025-08-29] DINOv3 éª¨å¹²ç½‘ç»œå—åˆ° Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) åº“å‘å¸ƒç‰ˆæœ¬æ”¯æŒï¼Œä»ç‰ˆæœ¬ [4.56.0](https://github.com/huggingface/transformers/releases/tag/v4.56.0) å¼€å§‹

[2025-08-14] DINOv3 éª¨å¹²ç½‘ç»œç°åœ¨åœ¨ [Hugging Face Hub](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) ä¸Šå¯ç”¨ï¼Œå¹¶å—åˆ° Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) åº“[å¼€å‘ç‰ˆæœ¬](https://github.com/huggingface/transformers/)æ”¯æŒ

# DINOv3 ğŸ¦–ğŸ¦–ğŸ¦–

**[Meta AI Research, FAIR](https://ai.meta.com/research/)**

Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, <br/>
Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, <br/>
Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, <br/>
TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, <br/>
Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, <br/>
Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski

[ :scroll: [`è®ºæ–‡`](https://arxiv.org/abs/2508.10104)] [ :newspaper: [`åšå®¢`](https://ai.meta.com/blog/dinov3-self-supervised-vision-model/)] [ :globe_with_meridians: [`ç½‘ç«™`](https://ai.meta.com/dinov3/)] [ :book: [`BibTeX`](#citing-dinov3)]

DINOv3 çš„å‚è€ƒ PyTorch å®ç°å’Œæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§ **[DINOv3](https://arxiv.org/abs/2508.10104)** è®ºæ–‡ã€‚

## æ¦‚è¿°

<div align="center">
  <img width="1364" height="1024" alt="market" src="https://github.com/user-attachments/assets/1411f491-988e-49cb-95ae-d03fe6e3c268" />

  <i></em><b>é«˜åˆ†è¾¨ç‡å¯†é›†ç‰¹å¾ã€‚</b><br/>æˆ‘ä»¬å¯è§†åŒ–ä½¿ç”¨ DINOv3 è¾“å‡ºç‰¹å¾è·å¾—çš„ä½™å¼¦ç›¸ä¼¼åº¦å›¾<br/>ï¼Œåœ¨æ ‡è®°ä¸ºçº¢è‰²åå­—çš„è¡¥ä¸ä¸æ‰€æœ‰å…¶ä»–è¡¥ä¸ä¹‹é—´ã€‚</i>
</div>

<br/>

ä¸€ä¸ªå¤šåŠŸèƒ½è§†è§‰åŸºç¡€æ¨¡å‹çš„æ‰©å±•å®¶æ—ï¼Œäº§ç”Ÿé«˜è´¨é‡çš„å¯†é›†ç‰¹å¾ï¼Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šå®ç°å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨å¹¿æ³›çš„è®¾ç½®ä¸­è¶…è¶Šä¸“é—¨çš„å…ˆè¿›æŠ€æœ¯ï¼Œæ— éœ€å¾®è°ƒ

## é¢„è®­ç»ƒæ¨¡å‹

:information_source: è¯·æŒ‰ç…§ä¸‹é¢æä¾›çš„é“¾æ¥è·å–æ‰€æœ‰æ¨¡å‹æƒé‡çš„è®¿é—®æƒé™ï¼šä¸€æ—¦è¢«æ¥å—ï¼Œå°†å‘é€ä¸€å°ç”µå­é‚®ä»¶ï¼Œå…¶ä¸­åŒ…å«æŒ‡å‘æ‰€æœ‰å¯ç”¨æ¨¡å‹æƒé‡ï¼ˆéª¨å¹²ç½‘ç»œå’Œé€‚é…å™¨ï¼‰çš„å®Œæ•´ URL åˆ—è¡¨ã€‚è¿™äº› URL å¯ä»¥ç”¨äºï¼š
- å°†æ¨¡å‹æˆ–é€‚é…å™¨æƒé‡ä¸‹è½½åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ `weights` æˆ– `backbone_weights` å‚æ•°å°† `torch.hub.load()` æŒ‡å‘è¿™äº›æœ¬åœ°æƒé‡ï¼Œæˆ–
- ç›´æ¥è°ƒç”¨ `torch.hub.load()` é€šè¿‡ `weights` æˆ– `backbone_weights` å‚æ•°ä»å…¶ URL ä¸‹è½½å’ŒåŠ è½½éª¨å¹²ç½‘ç»œæˆ–é€‚é…å™¨ã€‚

è¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ä»£ç ç‰‡æ®µã€‚

:warning: è¯·ä½¿ç”¨ `wget` è€Œä¸æ˜¯ç½‘ç»œæµè§ˆå™¨æ¥ä¸‹è½½æƒé‡ã€‚

åœ¨ç½‘é¡µæ•°æ®é›†ï¼ˆLVD-1689Mï¼‰ä¸Šé¢„è®­ç»ƒçš„ ViT æ¨¡å‹ï¼š
<table style="margin: auto">
  <thead>
    <tr>
      <th>æ¨¡å‹</th>
      <th>å‚æ•°</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-S/16 distilled </td>
      <td align="right">21M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-S+/16 distilled</td>
      <td align="right">29M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-B/16 distilled</td>
      <td align="right">86M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="right">300M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-H+/16 distilled</td>
      <td align="right">840M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-7B/16</td>
      <td align="right">6,716M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>

åœ¨ç½‘é¡µæ•°æ®é›†ï¼ˆLVD-1689Mï¼‰ä¸Šé¢„è®­ç»ƒçš„ ConvNeXt æ¨¡å‹ï¼š
<table style="margin: auto">
  <thead>
    <tr>
      <th>æ¨¡å‹</th>
      <th>å‚æ•°</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ConvNeXt Tiny</td>
      <td align="right">29M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Small</td>
      <td align="right">50M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Base</td>
      <td align="right">89M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Large</td>
      <td align="right">198M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>

åœ¨å«æ˜Ÿæ•°æ®é›†ï¼ˆSAT-493Mï¼‰ä¸Šé¢„è®­ç»ƒçš„ ViT æ¨¡å‹ï¼š
<table style="margin: auto">
  <thead>
    <tr>
      <th>æ¨¡å‹</th>
      <th>å‚æ•°</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="right">300M</td>
      <td align="center">SAT-493M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
    <tr>
      <td>ViT-7B/16</td>
      <td align="right">6,716M</td>
      <td align="center">SAT-493M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>


### é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œï¼ˆé€šè¿‡ PyTorch [Hub](https://docs.pytorch.org/docs/stable/hub.html)ï¼‰

è¯·æŒ‰ç…§[è¿™é‡Œ](https://pytorch.org/get-started/locally/)çš„è¯´æ˜å®‰è£… PyTorchï¼ˆåŠ è½½æ¨¡å‹æ‰€éœ€çš„å”¯ä¸€ä¾èµ–é¡¹ï¼‰ã€‚å¼ºçƒˆå»ºè®®å®‰è£…æ”¯æŒ CUDA çš„ PyTorchã€‚

```python
import torch

REPO_DIR = <PATH/TO/A/LOCAL/DIRECTORY/WHERE/THE/DINOV3/REPO/WAS/CLONED>

# åœ¨ç½‘é¡µå›¾åƒä¸Šé¢„è®­ç»ƒçš„ DINOv3 ViT æ¨¡å‹
dinov3_vits16 = torch.hub.load(REPO_DIR, 'dinov3_vits16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vits16plus = torch.hub.load(REPO_DIR, 'dinov3_vits16plus', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vitb16 = torch.hub.load(REPO_DIR, 'dinov3_vitb16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vitl16 = torch.hub.load(REPO_DIR, 'dinov3_vitl16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vith16plus = torch.hub.load(REPO_DIR, 'dinov3_vith16plus', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vit7b16 = torch.hub.load(REPO_DIR, 'dinov3_vit7b16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)

# åœ¨ç½‘é¡µå›¾åƒä¸Šé¢„è®­ç»ƒçš„ DINOv3 ConvNeXt æ¨¡å‹
dinov3_convnext_tiny = torch.hub.load(REPO_DIR, 'dinov3_convnext_tiny', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_convnext_small = torch.hub.load(REPO_DIR, 'dinov3_convnext_small', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_convnext_base = torch.hub.load(REPO_DIR, 'dinov3_convnext_base', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_convnext_large = torch.hub.load(REPO_DIR, 'dinov3_convnext_large', source='local', weights=<CHECKPOINT/URL/OR/PATH>)

# åœ¨å«æ˜Ÿå›¾åƒä¸Šé¢„è®­ç»ƒçš„ DINOv3 ViT æ¨¡å‹
dinov3_vitl16 = torch.hub.load(REPO_DIR, 'dinov3_vitl16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
dinov3_vit7b16 = torch.hub.load(REPO_DIR, 'dinov3_vit7b16', source='local', weights=<CHECKPOINT/URL/OR/PATH>)
```

### é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œï¼ˆé€šè¿‡ Hugging Face [Transformers](https://huggingface.co/docs/transformers/)ï¼‰

æ‰€æœ‰éª¨å¹²ç½‘ç»œéƒ½åœ¨ Hugging Face Hub çš„ [DINOv3](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) é›†åˆä¸­å¯ç”¨ï¼Œå¹¶é€šè¿‡ Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) åº“æ”¯æŒï¼ˆä»ç‰ˆæœ¬ 4.56.0 å¼€å§‹çš„å‘å¸ƒåŒ…ï¼‰ã€‚è¯·å‚è€ƒç›¸åº”çš„æ–‡æ¡£äº†è§£ç”¨æ³•ï¼Œä½†ä¸‹é¢æ˜¯ä¸€ä¸ªç®€çŸ­çš„ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ [Pipeline] æˆ– [AutoModel] ç±»è·å–å›¾åƒåµŒå…¥ã€‚

```python
from transformers import pipeline
from transformers.image_utils import load_image

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = load_image(url)

feature_extractor = pipeline(
    model="facebook/dinov3-convnext-tiny-pretrain-lvd1689m",
    task="image-feature-extraction", 
)
features = feature_extractor(image)
```

```python
import torch
from transformers import AutoImageProcessor, AutoModel
from transformers.image_utils import load_image

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = load_image(url)

pretrained_model_name = "facebook/dinov3-convnext-tiny-pretrain-lvd1689m"
processor = AutoImageProcessor.from_pretrained(pretrained_model_name)
model = AutoModel.from_pretrained(
    pretrained_model_name, 
    device_map="auto", 
)

inputs = processor(images=image, return_tensors="pt").to(model.device)
with torch.inference_mode():
    outputs = model(**inputs)

pooled_output = outputs.pooler_output
print("æ± åŒ–è¾“å‡ºå½¢çŠ¶:", pooled_output.shape)
```

å…¶ä¸­ä¸Šé¢çš„ `model` å’Œ `pretrained_model_name` å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š
- `facebook/dinov3-vits16-pretrain-lvd1689m`
- `facebook/dinov3-vits16plus-pretrain-lvd1689m`
- `facebook/dinov3-vitb16-pretrain-lvd1689m`
- `facebook/dinov3-vitl16-pretrain-lvd1689m`
- `facebook/dinov3-vith16plus-pretrain-lvd1689m`
- `facebook/dinov3-vit7b16-pretrain-lvd1689m`
- `facebook/dinov3-convnext-base-pretrain-lvd1689m`
- `facebook/dinov3-convnext-large-pretrain-lvd1689m`
- `facebook/dinov3-convnext-small-pretrain-lvd1689m`
- `facebook/dinov3-convnext-tiny-pretrain-lvd1689m`
- `facebook/dinov3-vitl16-pretrain-sat493m`
- `facebook/dinov3-vit7b16-pretrain-sat493m`

### å›¾åƒå˜æ¢

å¯¹äºä½¿ç”¨ LVD-1689M æƒé‡çš„æ¨¡å‹ï¼ˆåœ¨ç½‘é¡µå›¾åƒä¸Šé¢„è®­ç»ƒï¼‰ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å˜æ¢ï¼ˆæ ‡å‡† ImageNet è¯„ä¼°å˜æ¢ï¼‰ï¼š

```python
import torchvision
from torchvision.transforms import v2

def make_transform(resize_size: int = 256):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])
```


å¯¹äºä½¿ç”¨ SAT-493M æƒé‡çš„æ¨¡å‹ï¼ˆåœ¨å«æ˜Ÿå›¾åƒä¸Šé¢„è®­ç»ƒï¼‰ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å˜æ¢ï¼š


```python
import torchvision
from torchvision.transforms import v2

def make_transform(resize_size: int = 256):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.430, 0.411, 0.296),
        std=(0.213, 0.156, 0.143),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])
```

### é¢„è®­ç»ƒå¤´éƒ¨ - å›¾åƒåˆ†ç±»

<table style="margin: auto">
  <thead>
    <tr>
      <th>éª¨å¹²ç½‘ç»œ</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>å¤´éƒ¨<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">ImageNet</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>


ï¼ˆå®Œæ•´ï¼‰åˆ†ç±»å™¨æ¨¡å‹å¯ä»¥é€šè¿‡ PyTorch Hub åŠ è½½ï¼š

```python
import torch

# DINOv3
dinov3_vit7b16_lc = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_lc', source="local", weights=<DEPTHER/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)

```

### é¢„è®­ç»ƒå¤´éƒ¨ - åœ¨ SYNTHMIX æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ·±åº¦ä¼°è®¡å™¨

<table style="margin: auto">
  <thead>
    <tr>
      <th>éª¨å¹²ç½‘ç»œ</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>å¤´éƒ¨<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">SYNTHMIX</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>


```python
depther = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_dd', source="local", weights=<DEPTHER/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)
```

åœ¨å›¾åƒä¸Šè¿è¡Œæ·±åº¦ä¼°è®¡å™¨çš„å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
from PIL import Image
import torch
from torchvision.transforms import v2
import matplotlib.pyplot as plt
from matplotlib import colormaps

def get_img():
    import requests
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
    return image

def make_transform(resize_size: int | list[int] = 768):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])

depther = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_dd', source="local", weights=<DEPTHER/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)

img_size = 1024
img = get_img()
transform = make_transform(img_size)
with torch.inference_mode():
    with torch.autocast('cuda', dtype=torch.bfloat16):
        batch_img = transform(img)[None]
        batch_img = batch_img
        depths = depther(batch_img)

plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis("off")
plt.subplot(122)
plt.imshow(depths[0,0].cpu(), cmap=colormaps["Spectral"])
plt.axis("off")

```

#### å¤ç°è®ºæ–‡ç»“æœ

ç¡®ä¿ NYU æ•°æ®é›†æŒ‰ç…§[è¿™ä¸ª](DATASETS.md#depth-estimation-on-nyu)è®¾ç½®ã€‚

å¯åŠ¨ä»¥ä¸‹å‘½ä»¤ä»¥åœ¨ NYUv2 ä¸Šå¤ç°æˆ‘ä»¬è®ºæ–‡çš„æ·±åº¦ä¼°è®¡ç»“æœï¼Œä½¿ç”¨åœ¨ SYNTHMIX ä¸Šè®­ç»ƒçš„é¢„è®­ç»ƒæ·±åº¦ä¼°è®¡å™¨ï¼š

```shell
PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/depth/run.py \
config=dinov3/eval/depth/configs/config-nyu-synthmix-dpt-inference.yaml \
datasets.root=<PATH/TO/DATASET> \
load_from=dinov3_vit7b16_dd \
--output-dir <PATH/TO/OUTPUT/DIR>
```

æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœæ‚¨æƒ³åœ¨æ²¡æœ‰ dinov3.run.submit çš„æƒ…å†µä¸‹å¯åŠ¨ä»£ç ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ python ç›´æ¥æˆ– torchrunï¼š

```shell
PYTHONPATH=. python dinov3/eval/depth/run.py \
config=dinov3/eval/depth/configs/config-nyu-synthmix-dpt-inference.yaml \
datasets.root=<PATH/TO/DATASET> \
load_from=dinov3_vit7b16_dd \
output_dir=<PATH/TO/OUTPUT/DIR>
```

- ä¹Ÿå¯ä»¥ä½¿ç”¨ `result_config.save_results=true` ä¿å­˜é¢„æµ‹ç»“æœã€‚


#### åœ¨ NYUv2 Depth ä¸Šçš„çº¿æ€§æ·±åº¦ä¼°è®¡
```shell
PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/depth/run.py \
    model.dino_hub=dinov3_vit7b16 \
    config=dinov3/eval/depth/configs/config-nyu.yaml \
    datasets.root=<PATH/TO/DATASET> \
    --output-dir <PATH/TO/OUTPUT/DIR>
```

ä½œä¸šå®Œæˆåï¼Œæ‚¨å°†åœ¨æŒ‡å®šçš„è¾“å‡ºè·¯å¾„ç›®å½•ä¸­æ‰¾åˆ°
- `depth_config.yaml`ï¼ŒåŒ…å«æ‚¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„é…ç½®ï¼›
- `model_final.pth`ï¼Œè®­ç»ƒç»“æŸæ—¶çš„æœ€ç»ˆçº¿æ€§å¤´éƒ¨æ£€æŸ¥ç‚¹ï¼›å’Œ
- `results-depth.csv`ï¼ŒåŒ…å«æœ€ç»ˆæŒ‡æ ‡ã€‚

### é¢„è®­ç»ƒå¤´éƒ¨ - åœ¨ COCO2017 æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ£€æµ‹å™¨

<table style="margin: auto">
  <thead>
    <tr>
      <th>éª¨å¹²ç½‘ç»œ</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>å¤´éƒ¨<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">COCO2017</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>


```python
detector = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_de', source="local", weights=<DETECTOR/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)
```

### é¢„è®­ç»ƒå¤´éƒ¨ - åœ¨ ADE20K æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†å‰²å™¨

<table style="margin: auto">
  <thead>
    <tr>
      <th>éª¨å¹²ç½‘ç»œ</th>
      <th>é¢„è®­ç»ƒ<br/>æ•°æ®é›†</th>
      <th>å¤´éƒ¨<br/>æ•°æ®é›†</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">ADE20K</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a></td>
    </tr>
  </tbody>
</table>

```python
segmentor = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_ms', source="local", weights=<SEGMENTOR/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)
```

ä½¿ç”¨æä¾›çš„åˆ†å‰²å™¨ï¼ˆViT-7B + M2Fï¼‰åœ¨ ADE20K ä¸Šè¿è¡Œå®Œæ•´æ¨ç†çš„ç¤ºä¾‹å‘½ä»¤ï¼š

```shell
PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/segmentation/run.py \
config=dinov3/eval/segmentation/configs/config-ade20k-m2f-inference.yaml  \
datasets.root=<PATH/TO/DATASET> \
load_from=dinov3_vit7b16_ms \
--output-dir <PATH/TO/OUTPUT/DIR>
```

åœ¨å›¾åƒä¸Šè¿è¡Œåˆ†å‰²å™¨çš„å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
import sys
sys.path.append(REPO_DIR)

from PIL import Image
import torch
from torchvision import transforms
import matplotlib.pyplot as plt
from matplotlib import colormaps
from functools import partial
from dinov3.eval.segmentation.inference import make_inference


def get_img():
    import requests
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
    return image

def make_transform(resize_size: int | list[int] = 768):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])

segmentor = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_ms', source="local", weights=<SEGMENTOR/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)

img_size = 896
img  = get_img()
transform = make_transform(img_size)
with torch.inference_mode():
    with torch.autocast('cuda', dtype=torch.bfloat16):
        batch_img = transform(img)[None]
        pred_vit7b = segmentor(batch_img)  # åŸå§‹é¢„æµ‹  
        # å®é™…åˆ†å‰²å›¾
        segmentation_map_vit7b = make_inference(
            batch_img,
            segmentor,
            inference_mode="slide",
            decoder_head_type="m2f",
            rescale_to=(img.size[-1], img.size[-2]),
            n_output_channels=150,
            crop_size=(img_size, img_size),
            stride=(img_size, img_size),
            output_activation=partial(torch.nn.functional.softmax, dim=1),
        ).argmax(dim=1, keepdim=True)
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis("off")
plt.subplot(122)
plt.imshow(segmentation_map_vit7b[0,0].cpu(), cmap=colormaps["Spectral"])
plt.axis("off")
```




### é¢„è®­ç»ƒå¤´éƒ¨ - ä½¿ç”¨ `dino.txt` çš„é›¶æ ·æœ¬ä»»åŠ¡

<table style="margin: auto">
  <thead>
    <tr>
      <th rowspan="2">éª¨å¹²ç½‘ç»œ</th>
      <th>ä¸‹è½½</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="center">
        <a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[é“¾æ¥]</a>,
        <a href="https://dl.fbaipublicfiles.com/dinov3/thirdparty/bpe_simple_vocab_16e6.txt.gz">è¯æ±‡è¡¨</a>,
        <a href="https://dl.fbaipublicfiles.com/dinov2/thirdparty/LICENSE">è¯æ±‡è¡¨è®¸å¯è¯</a>
      </td>
    </tr>
  </tbody>
</table>

ï¼ˆå®Œæ•´ï¼‰dino.txt æ¨¡å‹å¯ä»¥é€šè¿‡ PyTorch Hub åŠ è½½ï¼š

```python
import torch
# DINOv3
dinov3_vitl16_dinotxt_tet1280d20h24l, tokenizer = torch.hub.load(REPO_DIR, 'dinov3_vitl16_dinotxt_tet1280d20h24l', weights=<SEGMENTOR/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)
```


## å®‰è£…

è®­ç»ƒå’Œè¯„ä¼°ä»£ç éœ€è¦ PyTorch ç‰ˆæœ¬ >= 2.7.1 ä»¥åŠä¸€äº›å…¶ä»–ç¬¬ä¸‰æ–¹åŒ…ã€‚è¯·æ³¨æ„ï¼Œä»£ç ä»…åœ¨æŒ‡å®šç‰ˆæœ¬ä¸‹è¿›è¡Œè¿‡æµ‹è¯•ï¼Œå¹¶ä¸”æœŸæœ›åœ¨ Linux ç¯å¢ƒä¸­è¿è¡Œã€‚è¦è®¾ç½®è®­ç»ƒå’Œè¯„ä¼°æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–é¡¹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è¯´æ˜ï¼š

*[micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)* **ï¼ˆæ¨èï¼‰** - å…‹éš†å­˜å‚¨åº“ï¼Œç„¶åä½¿ç”¨æä¾›çš„ç¯å¢ƒå®šä¹‰åˆ›å»ºå¹¶æ¿€æ´» `dinov3` conda ç¯å¢ƒï¼š

```shell
micromamba env create -f conda.yaml
micromamba activate dinov3
```

## å¼€å§‹ä½¿ç”¨

æä¾›äº†å‡ ä¸ª notebook æ¥å¼€å§‹åº”ç”¨ DINOv3ï¼š
- [è¡¥ä¸ç‰¹å¾çš„ PCA](notebooks/pca.ipynb)ï¼šæ˜¾ç¤º DINOv3 è¡¥ä¸ç‰¹å¾åœ¨å‰æ™¯å¯¹è±¡ä¸Šçš„ PCAï¼ˆè®ºæ–‡ä¸­çš„å½©è™¹å¯è§†åŒ–ï¼‰[[åœ¨ Google Colab ä¸­è¿è¡Œ]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/pca.ipynb)
- [å‰æ™¯åˆ†å‰²](notebooks/foreground_segmentation.ipynb)ï¼šåŸºäº DINOv3 ç‰¹å¾è®­ç»ƒçº¿æ€§å‰æ™¯åˆ†å‰²æ¨¡å‹ [[åœ¨ Google Colab ä¸­è¿è¡Œ]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb)
- [å¯†é›†å’Œç¨€ç–åŒ¹é…](notebooks/dense_sparse_matching.ipynb)ï¼šåŸºäº DINOv3 ç‰¹å¾åŒ¹é…ä¸¤ä¸ªä¸åŒå›¾åƒä¸Šå¯¹è±¡çš„è¡¥ä¸ [[åœ¨ Google Colab ä¸­è¿è¡Œ]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/dense_sparse_matching.ipynb)
- [åˆ†å‰²è·Ÿè¸ª](notebooks/segmentation_tracking.ipynb)ï¼šä½¿ç”¨åŸºäº DINOv3 ç‰¹å¾çš„éå‚æ•°æ–¹æ³•è¿›è¡Œè§†é¢‘åˆ†å‰²è·Ÿè¸ª [[åœ¨ Google Colab ä¸­è¿è¡Œ]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/segmentation_tracking.ipynb)

## æ•°æ®å‡†å¤‡

### ImageNet-1k

æ•°æ®é›†çš„æ ¹ç›®å½•åº”åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

- `<ROOT>/test/ILSVRC2012_test_00000001.JPEG`
- `<ROOT>/test/[..]`
- `<ROOT>/test/ILSVRC2012_test_00100000.JPEG`
- `<ROOT>/train/n01440764/n01440764_10026.JPEG`
- `<ROOT>/train/[...]`
- `<ROOT>/train/n15075141/n15075141_9993.JPEG`
- `<ROOT>/val/n01440764/ILSVRC2012_val_00000293.JPEG`
- `<ROOT>/val/[...]`
- `<ROOT>/val/n15075141/ILSVRC2012_val_00049174.JPEG`
- `<ROOT>/labels.txt`

æä¾›çš„æ•°æ®é›†å®ç°æœŸæœ›åœ¨é¢å¤–ç›®å½•ä¸‹å­˜åœ¨ä¸€äº›é¢å¤–çš„å…ƒæ•°æ®æ–‡ä»¶ï¼š

- `<EXTRA>/class-ids-TRAIN.npy`
- `<EXTRA>/class-ids-VAL.npy`
- `<EXTRA>/class-names-TRAIN.npy`
- `<EXTRA>/class-names-VAL.npy`
- `<EXTRA>/entries-TEST.npy`
- `<EXTRA>/entries-TRAIN.npy`
- `<EXTRA>/entries-VAL.npy`

è¿™äº›å…ƒæ•°æ®æ–‡ä»¶å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ Python ä»£ç ç”Ÿæˆï¼ˆä¸€æ¬¡ï¼‰ï¼š

```python
from dinov3.data.datasets import ImageNet

for split in ImageNet.Split:
    dataset = ImageNet(split=split, root="<ROOT>", extra="<EXTRA>")
    dataset.dump_extra()
```

è¯·æ³¨æ„ï¼Œæ ¹ç›®å½•å’Œé¢å¤–ç›®å½•ä¸å¿…æ˜¯ä¸åŒçš„ç›®å½•ã€‚

### ImageNet-22k

è¯·è°ƒæ•´[æ•°æ®é›†ç±»](dinov3/data/datasets/image_net_22k.py)ä»¥åŒ¹é…æ‚¨çš„æœ¬åœ°è®¾ç½®ã€‚

<br />

:warning: è¦æ‰§è¡Œä¸‹ä¸€èŠ‚ä¸­æä¾›çš„è®­ç»ƒå’Œè¯„ä¼°å‘½ä»¤ï¼Œ`dinov3` åŒ…åº”åŒ…å«åœ¨ Python æ¨¡å—æœç´¢è·¯å¾„ä¸­ï¼Œå³ç®€å•åœ°åœ¨è¦è¿è¡Œçš„å‘½ä»¤å‰åŠ ä¸Š `PYTHONPATH=.`ã€‚

## è®­ç»ƒ

### å¿«é€Ÿè®¾ç½®ï¼šåœ¨ ImageNet-1k ä¸Šè®­ç»ƒ DINOv3 ViT-L/16

åœ¨ 4 ä¸ª H100-80GB èŠ‚ç‚¹ï¼ˆ32 ä¸ª GPUï¼‰çš„ SLURM é›†ç¾¤ç¯å¢ƒä¸­ä½¿ç”¨ submitit è¿è¡Œ DINOv3 é¢„è®­ç»ƒï¼š

```shell
 PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 4 \
  --config-file dinov3/configs/train/vitl_im1k_lin834.yaml \
  --output-dir <PATH/TO/OUTPUT/DIR> \
  train.dataset_path=ImageNet22k:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```
è®­ç»ƒæ—¶é—´çº¦ä¸º 14 å°æ—¶ï¼Œç”Ÿæˆçš„æ£€æŸ¥ç‚¹åº”åœ¨ k-NN è¯„ä¼°ä¸Šè¾¾åˆ° 82.0%ï¼Œåœ¨çº¿æ€§è¯„ä¼°ä¸Šè¾¾åˆ° 83.5%ã€‚

è®­ç»ƒä»£ç æ¯ 12500 æ¬¡è¿­ä»£åœ¨è¯„ä¼°æ–‡ä»¶å¤¹ä¸­ä¿å­˜æ•™å¸ˆæƒé‡ä»¥ä¾›è¯„ä¼°ã€‚

### ç²¾ç¡®çš„ DINOv3 è®¾ç½®ï¼šè®­ç»ƒ DINOv3 ViT-7B/16

DINOv3 ViT-7B/16 åœ¨ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒã€‚è®­ç»ƒæ¶‰åŠ 3 ä¸ªé˜¶æ®µï¼š
- é¢„è®­ç»ƒ
- Gram é”šå®š
- é«˜åˆ†è¾¨ç‡é€‚åº”

#### é¢„è®­ç»ƒ

åœ¨ 32 ä¸ªèŠ‚ç‚¹ï¼ˆ256 ä¸ª GPUï¼‰çš„ SLURM é›†ç¾¤ç¯å¢ƒä¸­ä½¿ç”¨ submitit å¯åŠ¨ DINOV3 ViT-7B/16 é¢„è®­ç»ƒã€‚

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_pretrain.yaml \
  --output-dir <PATH/TO/OUTPUT/DIR> \
  train.dataset_path=<DATASET>:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```

#### Gram é”šå®š

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_gram_anchor.yaml \
  --output-dir <PATH/TO/OUTPUT/DIR> \
  train.dataset_path=<DATASET>:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET> \
  gram.ckpt=<PATH/TO/GRAM_TEACHER_FROM_PREVIOUS_STEP>   
```

#### é«˜åˆ†è¾¨ç‡é€‚åº”


```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_high_res_adapt.yaml \
  --output-dir <PATH/TO/OUTPUT/DIR> \
  train.dataset_path=<DATASET>:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET> \
  gram.ckpt=<PATH/TO/TEACHER_FROM_GRAM> \
  student.resume_from_teacher_chkpt=<PATH/TO/TEACHER_FROM_GRAM>
```

## å¤šè’¸é¦ 

### æµ‹è¯•è®¾ç½®ï¼š

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 1 \
  --config-file dinov3/configs/train/multi_distillation_test.yaml \
  --output-dir <PATH/TO/OUTPUT/DIR> \
  --multi-distillation \
  train.dataset_path=<DATASET>:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```

## è¯„ä¼°

è®­ç»ƒä»£ç å®šæœŸä¿å­˜æ•™å¸ˆæƒé‡ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹ï¼Œåœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œä»¥ä¸‹è¯„ä¼°ï¼š


### åœ¨ ImageNet-1k ä¸Šçš„é€»è¾‘å›å½’åˆ†ç±»

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/log_regression.py \
  model.config_file=<PATH/TO/OUTPUT/DIR>/config.yaml \
  model.pretrained_weights=<PATH/TO/OUTPUT/DIR>/teacher_checkpoint.pth \
  output_dir=<PATH/TO/OUTPUT/DIR> \
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET> \
  eval.test_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```

### åœ¨ ImageNet-1k ä¸Šçš„ k-NN åˆ†ç±»

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/knn.py \
  model.config_file=<PATH/TO/OUTPUT/DIR>/config.yaml \
  model.pretrained_weights=<PATH/TO/OUTPUT/DIR>/teacher_checkpoint.pth \
  output_dir=<PATH/TO/OUTPUT/DIR> \
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET> \
  eval.test_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```

### åœ¨ ImageNet-1k ä¸Šä½¿ç”¨æ•°æ®å¢å¼ºçš„çº¿æ€§åˆ†ç±»

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/linear.py \
  model.config_file=<PATH/TO/OUTPUT/DIR>/config.yaml \
  model.pretrained_weights=<PATH/TO/OUTPUT/DIR>/teacher_checkpoint.pth \
  output_dir=<PATH/TO/OUTPUT/DIR> \
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET> \
  train.val_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET>:extra=<PATH/TO/DATASET>
```

### åœ¨ ADE20K ä¸Šä½¿ç”¨æ•°æ®å¢å¼ºçš„çº¿æ€§åˆ†å‰²

```shell
PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/segmentation/run.py \
model.dino_hub=dinov3_vit7b16 \
config=dinov3/eval/segmentation/configs/config-ade20k-linear-training.yaml \
datasets.root=<PATH/TO/DATASET> \
--output-dir <PATH/TO/OUTPUT/DIR>
```

ä½œä¸šå®Œæˆåï¼Œæ‚¨å°†åœ¨æŒ‡å®šçš„è¾“å‡ºè·¯å¾„ç›®å½•ä¸­æ‰¾åˆ°
- `segmentation_config.yaml`ï¼ŒåŒ…å«æ‚¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„é…ç½®ï¼›
- `model_final.pth`ï¼Œè®­ç»ƒç»“æŸæ—¶çš„æœ€ç»ˆçº¿æ€§å¤´éƒ¨æ£€æŸ¥ç‚¹ï¼›å’Œ
- `results-semantic-segmentation.csv`ï¼ŒåŒ…å«æœ€ç»ˆæŒ‡æ ‡ã€‚

### ä½¿ç”¨ dino.txt åœ¨ DINOv3 ä¸Šè¿›è¡Œæ–‡æœ¬å¯¹é½

æ–‡æœ¬å¯¹é½å¯ä»¥æŒ‰ç…§ `dino.txt` å³ [DINOv2 Meets Text](https://arxiv.org/abs/2412.16334) çš„æ–¹æ³•è¿›è¡Œã€‚

```shell
PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/text/train_dinotxt.py \
   --nodes 4 \
  # æ–‡æœ¬å¯¹é½çš„ç¤ºä¾‹é…ç½®åœ¨è¿™é‡Œï¼šdinov3/eval/text/configs/dinov3_vitl_text.yaml \ 
  trainer_config_file="<PATH/TO/DINOv3/TEXT/CONFIG>" \
  output-dir=<PATH/TO/OUTPUT/DIR>
```
å¯åŠ¨ä¸Šè¿°å‘½ä»¤åœ¨ 4 ä¸ªèŠ‚ç‚¹ä¸Šè®­ç»ƒæ–‡æœ¬å¯¹é½ï¼Œæ¯ä¸ªèŠ‚ç‚¹ 8 ä¸ª gpuï¼ˆæ€»å…± 32 ä¸ª gpuï¼‰ã€‚
è¯·æ³¨æ„ï¼ŒDINOv3 è®ºæ–‡ä¸­çš„æ–‡æœ¬å¯¹é½æ¨¡å‹æ˜¯åœ¨ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œè¿™é‡Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨ ```CocoCaptions``` æ•°æ®é›†çš„ç¤ºä¾‹é…ç½® ```dinov3/eval/text/configs/dinov3_vitl_text.yaml``` ç”¨äºè¯´æ˜ç›®çš„ã€‚
è¯·è°ƒæ•´æä¾›çš„ ```CocoCaptions``` æ•°æ®é›†ç±»ï¼Œæ•°æ®é›†å¯ä»¥åœ¨[è¿™é‡Œ](https://www.kaggle.com/datasets/nikhil7280/coco-image-caption)æ‰¾åˆ°  

## è®¸å¯è¯

DINOv3 ä»£ç å’Œæ¨¡å‹æƒé‡åœ¨ DINOv3 è®¸å¯è¯ä¸‹å‘å¸ƒã€‚æœ‰å…³å…¶ä»–è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [LICENSE.md](LICENSE.md)ã€‚

## è´¡çŒ®

è¯·å‚è§[è´¡çŒ®](CONTRIBUTING.md)å’Œ[è¡Œä¸ºå‡†åˆ™](CODE_OF_CONDUCT.md)ã€‚

## å¼•ç”¨ DINOv3

å¦‚æœæ‚¨å‘ç°æ­¤å­˜å‚¨åº“æœ‰ç”¨ï¼Œè¯·è€ƒè™‘ç»™ä¸€ä¸ªæ˜Ÿæ ‡ :star: å’Œå¼•ç”¨ :t-rex::

```
@misc{simeoni2025dinov3,
  title={{DINOv3}},
  author={Sim{\'e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\"e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  year={2025},
  eprint={2508.10104},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2508.10104},
}
```
